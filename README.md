Here's the updated README file with the additional information about the embedding models and LLMs used:

---

# RAG Evaluation with LlamaIndex and Google Gemini

This project evaluates the performance of a Retrieval-Augmented Generation (RAG) application using the **LlamaIndex** framework and various **embedding models** along with **large language models (LLMs)**. The system is assessed using the metrics of **faithfulness**, **relevancy**, and **correctness** to ensure high-quality responses to user queries.

## Key Features:
1. **RAG Architecture**: Uses LlamaIndex for managing documents and retrieving relevant information, integrated with multiple LLMs to generate responses.
2. **Embedding Models**:
   - **Bgesmall**
   - **GeminiEmbedding**
   - **OpenAIEmbedding**
   - **Jina**
   
   These embedding models are used to create vector representations of the document content, facilitating efficient and relevant information retrieval.
3. **Large Language Models (LLMs)**:
   - **OpenAI GPT-3.5**
   - **OpenAI GPT-4**
   - **Mistral**
   - **Google Gemini**
   
   The performance of these LLMs is compared in terms of their ability to generate accurate and relevant answers.
4. **Evaluation Metrics**:
   - **Faithfulness**: Measures how closely the generated response adheres to the source content.
   - **Relevancy**: Assesses how closely the response matches the user's query.
   - **Correctness**: Evaluates the factual accuracy of the generated response.

5. **Performance Comparison**: The system compares the performance of different LLMs (GPT-3.5, GPT-4, Mistral, and Gemini) and embedding models (Bgesmall, GeminiEmbedding, OpenAIEmbedding, and Jina) based on the defined evaluation metrics.

## How It Works:
1. **Document Upload and Embedding**: The system accepts a PDF document, processes it into chunks, and stores it in a vector database using one of the embedding models for efficient retrieval.
2. **Query Answering**: Users can input questions related to the uploaded document, and the system generates responses using the selected LLM.
3. **Evaluation**: The generated responses are evaluated against predefined questions and reference answers using metrics for faithfulness, relevancy, and correctness.
4. **Model Comparison**: After generating responses, the performance of each LLM is compared to identify which model provides the most accurate and relevant answers.

## Steps to Run:
1. Clone the repository and navigate to the project directory:
   ```bash
   git clone <repository_url>
   cd RAG_Evaluation_with_LlamaIndex
   ```
2. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Open the Jupyter Notebook:
   ```bash
   jupyter notebook RAG_Evaluation_with_LlamaIndex.ipynb
   ```
4. Upload your documents and run the cells to:
   - Process the document using one of the embedding models.
   - Ask questions related to the document content.
   - Evaluate and compare the answers generated by different LLMs.

## Evaluation Metrics:
- **Faithfulness Score**: How accurately the response follows the source content (e.g., Score: 1.0).
- **Relevancy Score**: How relevant the response is to the userâ€™s query (e.g., Score: 1.0).
- **Correctness Score**: How factually correct the response is (e.g., Score: 1.0).

## Results:
The project compares the performance of different embedding models and LLMs, providing insight into which combinations of models deliver the best results in terms of faithfulness, relevancy, and correctness.

## Conclusion:
This project demonstrates the effectiveness of different embedding models and LLMs in the context of RAG systems. By comparing the performances of models like GPT-3.5, GPT-4, Mistral, and Gemini, as well as various embedding techniques, this evaluation provides key insights into the capabilities of different models in answering document-based queries accurately.

---

This README now includes details on the embedding models and LLMs used, as well as the performance comparison. Let me know if you'd like any further adjustments!
